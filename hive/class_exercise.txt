-- download the files in the location https://drive.google.com/drive/folders/0B0MdkEsxQHAQeFB6WHMzWEx6eGM?usp=sharing
-- download price_data.txt and price_gap_data.txt

Scenario: You are provided with two files in a local drive. Create copies of each file in their respective folders on hdfs.

(Objective) Load data into and out of HDFS using the Hadoop File System (FS) commands (Note that this is not important for the exam as the files will already by provided for you and you always have HUE web console to help you will the utility functions)
==========================================================================
#create two folders in hdfs 
hdfs dfs -mkdir -p /user/cloudera/rawdata/hist_forex/price_data
hdfs dfs -mkdir -p /user/cloudera/rawdata/hist_forex/gap_data

#copy the data files from your local drive to hdfs
hdfs dfs -put all_price_data.txt /user/cloudera/rawdata/hist_forex/price_data
hdfs dfs -put price_gap_data.txt /user/cloudera/rawdata/hist_forex/gap_data

Outcome
===============================
Now you should have both the files and folders in place.





Scenario: Create a hive table from the all_price_data.txt dataset and run sample select queries.

(Objective) Read and/or create a table in the Hive metastore in a given schema
====================================================================
create external table raw_price_data (datetimestamp string, open float, high float, low float, close float, volume int, sym string)
row format delimited
fields terminated by ';'
location '/user/cloudera/rawdata/hist_forex/price_data';

#sample queries
--return all datatimestamp, open, close and sym for all instances of XUAUSD traded.
select datetimestamp, open, close, sym from raw_price_data where sym = 'XUAUSD';
--return all trading instrument symbols in this dataset
select distinct sym from raw_price_data;
-- return the avg open and close prices for all trading pairs
select sym, avg(open) avg_open, avg(close) avg_close from raw_price_data group by sym;




Scenario: Using the raw_price_data, dataset create a wider table called price_data_wide that splits the datatimestamp column into year, month, day, hour, min integer columns. The new table must be stored in parquet format

(Objective) Read and/or create a table in the Hive metastore in a given schema
==============================================================================
create external table price_data_wide (year int, month int, day int, hour int, min int, open float, high float, low float, close float, volume int, sym string)
stored as parquet
location '/user/cloudera/rawdata/hist_forex/price_data_wide';

--load data from raw_price_data
insert overwrite table price_data_wide
select cast(substr(datetimestamp, 0, 4) as int) year, cast(substr(datetimestamp, 5, 2) as int) month, 
cast(substr(datetimestamp, 7, 2) as int) day, cast(substr(datetimestamp, 10, 2) as int) hour, cast(substr(datetimestamp, 12, 2) as int) min 
, open, high, low, close, volume, sym
from raw_price_data

#sample query
-- return the monthly average volatility (differnce between high and low) for all dollar pairs in the year 2010 
select year, month, sym, avg(high-low) avg_volatility from price_data_wide where year = 2010 and sym like '%USD%' group by year, month, sym 




Scenario: Using the raw_price_data, dataset create a wider table called price_data_avro that splits the datatimestamp column into year, month, day, hour, min integer columns. The new table must be stored in avro format using a /user/cloudera/rawdata/hist_forex/schema/price_data.avsc schema file provide to you. All records in the raw_price_data must be successfully migrated to price_data_avro

(Objective) Create a table in the Hive metastore using the Avro file format and an external schema file
==============================================================================
create external table price_data_avro (year int, month int, day int, hour int, min int, open float, high float, low float, close float, sym string)
stored as avro
location '/user/cloudera/rawdata/hist_forex/price_data_avro'
tblproperties('avro.schema.url'='hdfs://quickstart.cloudera:8020/user/cloudera/rawdata/hist_forex/schema/price_data.avsc');

--load data from raw_price_data
insert overwrite table price_data_avro
select cast(substr(datetimestamp, 0, 4) as int) year, cast(substr(datetimestamp, 5, 2) as int) month, 
cast(substr(datetimestamp, 7, 2) as int) day, cast(substr(datetimestamp, 10, 2) as int) hour, cast(substr(datetimestamp, 12, 2) as int) min 
, open, high, low, close, sym
from raw_price_data;

#sample query
-- return the monthly average volatility (differnce between high and low) for all dollar pairs in the year 2010 
select year, month, sym, avg(high-low) avg_volatility from price_data_wide where year = 2010 and sym like '%USD%' group by year, month, sym;




Scenario: Assume that quant analysis do their analysis mainly on data in a year window. You are asked to partition all sales data by year and sym and load all dataset from the data_price_wide table into it. The new table should be named data_price_part. Also because analyst discovered that the volume column is always null, you have been asked to remove it from the new table.

(Objective): Improve query performance by creating partitioned tables in the Hive metastore
=====================================================================================================
create external table price_data_part (month int, day int, hour int, min int, open float, high float, low float, close float)
partitioned by (year int, sym string)
stored as parquet
location '/user/cloudera/rawdata/hist_forex/price_data_part';

--load data from price_data_wide
--first enable dynamic partitioning
set hive.exec.dynamic.partition=enable;
--set the mode of dynamic partition to nonstrict
set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table price_data_part partition (year, sym)
select month, day, hour, min, open, high, low, close, year, sym
from price_data_wide


